---
title: 'Final Assignment: Predicting Director Compensation'
author: "Jasper Ginn (s6100848)"
date: "April 8, 2019"
output:
  pdf_document: 
    number_sections: false
header-includes:
 \usepackage{float}
 \restylefloat{table}
 \usepackage{setspace}
 \usepackage{xcolor}
 \usepackage[font={small}]{caption}
 \singlespacing
 \floatplacement{figure}{H}
---
<!-- Set font size -->
\fontsize{11}{22}

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos="H", fig.width = 7, fig.height = 4, echo = FALSE, 
                      fig.align="center", warning = FALSE, message = FALSE, eval=FALSE)

rm(list=ls())

# Load blm package
library(blm)

# Load ggplot
library(ggplot2)
library(gridExtra)

# Get the directors data
data("directors")

# Keep for later
dirs <- directors

# Preprocess directors data
# This is a subset of the original (proprietary) dataset
# 5 sectors have not been included due to bimodality in the data. 
# See the folder 'data-raw' for the script used to create the directors data.
library(dplyr)
directors <- directors %>%
  # Log compensation
  mutate(Compensation = log(Compensation)) %>%
  # Subtract mean from Age variable
  mutate(Age = Age - mean(Age),
         Male = as.numeric(Male) - mean(as.numeric(Male)))

# Load models 
# i.e. prevent R from re-running the models the entire time when knitting the doc (this takes a long time).
# Instead, run code line-by-line if you need to.
dirmod1 <- readRDS("models/model1.rds")
dirmod2 <- readRDS("models/model2.rds")
resm2 <- readRDS("models/model3.rds")
```

```{r julia setup}
# Set up blm julia
blm_setup() 
```

# Introduction

Problem statement, literature & hypotheses

# Differences in Bayesian and Frequentist inference

https://en.wikipedia.org/wiki/Probability_interpretations

## Definition of probability and the source of uncertainty 

For a large part, differences between Frequentist and Bayesian inference stems from their respective view of uncertainty and how this is captured by probabilities [CITE]. In the Frequentist framework, probabilities are looked upon as the limiting value of the number of $k$ successes in a sequence of $n$ trials [CITE], or:

$$
\tag{1}
\begin{aligned}
p = \lim_{n \to \infty} \frac{k}{n}
\end{aligned}
$$

The implications of this definition are that (1) probabilities make sense only in the context of infinite trials, and (2) the probability is *fixed* in the population and our uncertainty about it reduces as we repeatedly take more (or larger) samples. In particular, this definition implies that the only source of randomness by which our estimate $\hat{p}$ differs from the true value $p$ comes from the data [CITE], which may differ from sample to sample due to, for example, sampling error.

The Bayesian framework looks upon probabilities as a means of quantifying uncertainty about knowledge [CITE]. Even though the 'true' parameter value may be fixed, we are limited by our knowledge of this value. Hence, the uncertainty by which we make statements about the world changes as we collect more information which is represented by the posterior distribution, which may be looked upon very loosely as the collection 'true' values conditional on how certain we are about the veracity of this knowledge.

## Epistemological differences about what constitutes knowledge

In Frequentist statistical inference, evidence may originate only from the data. This is not the case in Bayesian statistical inference, where inferences are based on a mix of domain expertise (*prior* or *belief*) and evidence from the data. This makes perfect sense in the Bayesian framework; if we are uncertain about our knowledge then we can constrain the parameter space by injecting what we *do* know. Hence, a Bayesian looks upon prior beliefs as just another source of knowledge that has been translated into a probability density. The exact definition of the prior belief affects the results to the degree to which we feel certain of its veracity. This is illustrated in the figure below: a strong belief in our prior knowledge (represented by a small variance) leads to a more constrained posterior.

```{r calculate prior variance effect}
# Posterior shrinkage
dirmod3 <- blm("Compensation ~ Sector + Age + Male",
                data=directors) %>%
  # Need to delete posterior when we change settings
  # (only has an effect if posterior already present)
  delete_posterior() %>%
  # Set an informative prior on the coefficients
  # Male (b4) is speculated to have 17% pay increase over women
  set_prior("b4", mu = .17, sd=0.001) %>%
  # Update sampling settings
  set_sampling_options(chains = 2, iterations = 20000,
                       burn = 4000, thinning=3) %>%
  # Set initial values
  # If we draw from the priors the starting values will be too large
  # This is an issue for MH because it takes many more iterations to converge
  set_initial_values(chain_1 = list("b" = c(4, 2, 3, 8, 10), "sigma"= 1),
                     chain_2 = list("b" = c(2, 4, 7, 2, 3), "sigma"=2)) %>%
  # Sample the posterior distribution
  sample_posterior()

# Change prior and run for another 600000 observations
dirmod3 <- dirmod3 %>%
  set_prior("b4", mu = .17, sd=0.01) %>%
  update_posterior(iterations=20000)
# Same
dirmod3 <- dirmod3 %>%
  set_prior("b4", mu = .17, sd=0.1) %>%
  update_posterior(iterations=20000)

# Save
saveRDS(dirmod3, "data/model_effects_prior.rds")
```

```{r effect of prior variance, fig.height=2, fig.width=7, fig.cap="Effect of adjusting the prior variance of the coefficient for variable Male (a) on the posterior distribution (b). The mean is set at M=.17 which represents a 17 percent increase in compensation for males versus females. Reducing the prior variance represents increased certainty about the estimate of our domain knowledge and weights the information from the data less strongly compared to domain knowledge. The dashed lines indicate the value of the posterior means.", eval=TRUE}

dirmod3 <- readRDS("data/model_effects_prior.rds")
# Retrieve posterior samples for male variable, burn and label by prior variance 
library(purrr)
pd <- get_value(dirmod3, "posterior") %>%
  get_value("samples") %>%
  map(function(x) {
    r1 <- x[4001:20000,"Male"]
    r2 <- x[24001:40000, "Male"]
    r3 <- x[44001:60000, "Male"]
    return(
      data.frame(
        "Male" = c(r1, r2, r3),
        "Prior" = factor(c(rep(1, length(r1)),rep(2, length(r2)), rep(3, length(r3))),
                         labels = c("0.001", "0.01", "0.1"))
      )
    )
  }) %>%
  bind_rows()

# Calculate SD by prior
pd_sd <- pd %>%
  group_by(Prior) %>%
  summarize(psd = sd(Male),
            pmn = mean(Male))

# Posterior z-value
zscore <- function(mean, sd, theta) (mean - theta) / sd
z1 <- zscore(pd_sd$pmn[1], pd_sd$psd[1], .17)
z2 <- zscore(pd_sd$pmn[2], pd_sd$psd[2], .17)
z3 <- zscore(pd_sd$pmn[3], pd_sd$psd[3], .17)

# Posterior scaling of sd
ps1 <- pd_sd$psd[1]^2 / as.numeric(levels(pd_sd$Prior)[1])^2 # 841
ps2 <- pd_sd$psd[2]^2 / as.numeric(levels(pd_sd$Prior)[2])^2 # 35
ps3 <- pd_sd$psd[3]^2 / as.numeric(levels(pd_sd$Prior)[3])^2 # 0.53

# Posterior means
pms <- round(pd_sd$pmn, digits=3)

library(ggplot2)
library(ggpubr)
priorvar <- ggplot(data.frame(
  "Male" = c(rnorm(10000, mean=.17, sd=0.001), 
             rnorm(10000, mean=.17, sd=.01), 
             rnorm(10000, mean=.17, sd=.1)),
  "Prior" = factor(c(rep(1, 10000), rep(2, 10000), rep(3, 10000)), labels = c("0.001", "0.01", "0.1"))
  ),
  aes(x=Male, fill=Prior)) +
  geom_histogram(alpha=0.4, position="identity", color="black") +
  theme_blm(text_size = 9) +
  theme(axis.title = element_blank(),
        axis.text.y=element_blank()) +
  scale_fill_brewer(palette = "Set1",
                    name="Prior variance") +
  labs(title = "(a) Prior distribution")

# Posterior distribution
postvar <- ggplot(pd, aes(x=Male, fill=Prior)) +
  geom_histogram(alpha=0.4, position="identity", color="black") +
  theme_blm(text_size = 9) +
  theme(axis.title = element_blank(),
        axis.text.y=element_blank()) +
  geom_vline(xintercept = pms[1], color=RColorBrewer::brewer.pal(3, "Set1")[1], 
             linetype="dashed", size=1.2) +
  geom_vline(xintercept = pms[2], color=RColorBrewer::brewer.pal(3, "Set1")[2], 
             linetype="dashed", size=1.2) +
  geom_vline(xintercept = pms[3], color=RColorBrewer::brewer.pal(3, "Set1")[3], 
             linetype="dashed", size=1.2) +
  scale_fill_brewer(palette = "Set1") +
  labs(title= "(b) Posterior distribution")

# Arrange plots with common legend
ggarrange(priorvar, postvar, ncol=2, common.legend=TRUE, legend="top")
```

The claim that the Frequentist approach is more objective would hold only if the data collection process, (pre-)processing steps and analysis are guaranteed to be objective. This is a tenuous assumption at best in the social sciences; the way in which we collect data is fraught with subjective decisions during data collection, manipulation and analysis [CITE]. Data, in and of itself, is not objective [CITE], and the critique leveled at Bayesians boils down to the practice of incorporating domain knowledge *explicitly* through the use of a prior.

## Methods of estimation and hypothesis testing

Whether a statistician regards the data or the parameters as a random variable determines their choice of estimation method. A Frequentist, believing that variation originates from data, will want to optimize the 'likelihood' of the data conditional on the parameters. That is, we find the *most likely combination* of parameters $\hat{\theta}$ that explain the data and that provide consistent and asymptotically unbiased estimators, or:

$$
\tag{1}
\hat{\theta} = \arg \max_{\theta \in \Theta} \text{Log Likelihood}(\theta |\text{data}) 
$$

Conversely, given that a Bayesian thinks of the data as fixed and the parameters as random variables, they are interested in finding the distribution of the parameters and hence the source of variance:

$$
\tag{2}
p(\theta | \text{data}) = \frac{p(\text{data}|\theta) p(\theta)}{p(\text{data})}
$$

The question of how the definition of probability and the methods of estimation impact inference under these frameworks is illustrated by the difference in interpretation of the confidence interval and the credible interval. When we calculate the confidence interval [REFER TO CENTRAL LIMIT THEOREM/FREQUENCY OF SAMPLING MEANS], the boundaries of the confidence intervals are interpreted as random variables due to sampling error since they estimate the frequency of sampling means. With a credible interval, we do not have this source of variability, which gives rise to the definition that the parameter is contained in the credible interval with some probability because the parameter space is assumed to be known under the assumptions by which we arrived at the posterior distribution [CHECK].

Further implications are to be found in the way we test hypotheses in these frameworks. In the Frequentist framework, we usually partition parameter space into an acceptance and a rejection region based on some null and alternative hypothesis. On the basis of a test statistic, computed from the data, we then decide whether or not the result we observe is likely to occur due to chance.

* Type I && II
* Test statistic
* probability of a hypothesis given the data

This is not so in the Bayesian Framework. Given that the data or the evidence can be viewed as a means to update a prior belief, hypothesis testing focuses on the degree to which the evidence found in the data supports this initial belief.

--> Probability of the null hypothesis (p 161, 162 Berger & Benny). "Flipping" of the hypothesis 

- Endless transformations on posterior distribution ==> not possible in frequentist except when calculating mean/variances ==> see freq cartoon book

- Frequentists obscure the interpretation process, Bayesian obscure the posterior distribution collection process

Frequentist statistics obscures the interpretation of critical statistics to the point where students learn the heuristics ('p-value is significant') before truly understanding what that heuristic means. Conversely, Bayesian statistics provides an explicit description of a model and its assumptions and has an intuitive interpretation of statistical results. However, the approach obfuscates the estimation method by using the arcane process of Markov Chain Monte Carlo (MCMC) sampling. 

<!-- Legacy
The effect of changing the prior variance can be summarized using a posterior shrinking factor (CITE BETANCOURT) and posterior z-score. The shrinking factor shows us the factor by which the variance of the posterior distribution shrinks or expands compared to the prior variance. The posterior z-score tells us the direction and magnitude by which the posterior mean shifts compared to the prior mean; if we are confident in the precision of our domain knowledge (resulting in small prior variance), the resulting posterior weights this information strongly and we end up with a posterior mean that lies closer to the prior mean (represented by the z-score). However, given that we have small variance, and if this is not corroborated by the data, then the shrinkage factor will be large. 
-->

# Methods & Results

Table XX below shows the descriptive statistics for each variable. The outcome variable **compensation** is given in thousands of Great British pounds and has been log-transformed.

```{r descriptive statistics, eval=FALSE, results='asis'}
out <- capture.output(stargazer::stargazer(dirs, summary=TRUE, median=TRUE))
cat(paste0(out[-1:-3], "\n"))
```

The model is given by the following equation:

$$
\begin{aligned}
\hat{\log(\text{compensation}_i)} = 
&\beta_0 + \beta_{1} \text{age}_i + \beta_2\text{male}_i + \beta_3\text{SectorServices}_i + \beta_4\text{SectorBasicMaterials}_i + \epsilon_i
\end{aligned}
$$

Given that we measure the compensation on a log scale, the coefficients we derive from the model must be interpreted as percentages. This forces us to rethink the priors we set on the model (default is mu=0, sd=1000). Additionally, even if we know very little of the effect of age on compensation (or gender for that matter), we can set some reasonable assumptions in terms of upper and lower bounds. For age, we now assume that, as age increases, compensation increases as well (not a crazy assumption). But, given that we are not certain about the extent to which it increases, we set the standard deviation of this estimate to $.1$, representing a spread of approximately $10\%$^[For values close to $0$, $exp(x) \approx 1 + x$]. For gender, we know from earlier studies that the gender gap is $17\%$. However, theory suggests that the gender gap is lower or non-existent at top-tier firms. Hence, we set this prior a mean of $.05$ with a spread of $.03$, which reflects our uncertainty of the estimate. 

Our hypotheses are:

$$
\text{H}_1\text{: } \beta_{\text{Male}} \approx 0
$$

$$
\text{H}_2\text{: } \beta_{\text{Age}} > 0 
$$

$$
\text{H}_3\text{: } \beta_{\text{Financial}} > \beta_{\text{Basic Materials}} > \beta_{\text{Services}}
$$

$$
\text{H}_u\text{: } \beta_\text{Male}, \beta_{\text{Age}}
$$

These hypotheses will be included in the models we construct. (Hoitjtink p.24).

PRA is the weight of a particular Bayes factor compared to the other hypotheses. That is, it is a proportion of the total evidence.

PRB is the probability of the marginal likelihood of each hypothesis, or $P(H_i | model)$. In essence, this functions as an indication of error probabilities; if we 'cut' the parameter space into many pieces, so does our confidence in our hypothesis go down.

The quantities PRB are the Bayesian error probabilities 

BF sensitive to outliers (Hoijtink pp.31-32) ==> check for outliers before BF. Also violation of model assumptions using ppc. We should use a robust BF but we don't have that. Anyway, interpret the BF with a grain of salt.

Bayes' Factor is sensitive to the choice of prior ==> something about the size of complexity. (Hoijtink pp.29-30)

```{r model1, include = FALSE, eval=FALSE}
## Run two linear models with blm.

# Model 1: only age and male dummy ==> baseline model -----

# Priors are uninformative (mean 0, sd 1000)
# Initial values are drawn from the priors
dirmod1 <- blm("Compensation ~ Age + Male",
                    data=directors) %>%
  # Set sampling options
  set_sampling_options(chains = 2, iterations = 30000,
                       burn = 8000, thinning=3) %>%
  # Set initial values
  set_initial_values(chain_1 = list("b" = c(7, -5, 0), "sigma" = 1),
                     chain_2 = list("b" = c(-5, 0, 7), "sigma" = 2)) %>%
  # Set priors
  set_prior("b1", mu=0, sd=.1) %>%
  set_prior("b2", mu=.05, sd=.1) %>%
  # Compute null model as well
  compute_null_model() %>%
  # Set hypotheses
  # What does this formulation mean in terms of standardized coefficients?
  set_hypothesis("H1", "|b0 - (b0 + b2)| < .01") %>%
  # Sample
  sample_posterior() %>%
  # Compute posterior predictive checks (using a random sample of 80% of the data)
  evaluate_ppc(p=.8) %>%
  # Evaluate hypotheses
  evaluate_hypotheses()

# Sanity check for coefficients ==> compare to linear model
coef(dirmod1)
coef(lm("Compensation ~ Age + Male", data=directors))

# Check convergence for intercept-only model
plot(dirmod1, "nullmodel") # OK

# Inspect convergence
dirmod1 %>%
  evaluate_convergence_diagnostics() # OK (GR statistic)

# Plots
plot(dirmod1, "history") # Ok
plot(dirmod1, "density") # OK
plot(dirmod1, "autocorrelation") # OK

# Summary
summary(dirmod1)
# DIC Null model = 5.13 
# DIC = 508
# R2 = low (approx 2.1%)
# Approx. model BF == .0915
# ==> male is a predictor of logged compensation (CCI contains 0)

# Plot R-squared 
# See: https://www.tandfonline.com/doi/abs/10.1080/00031305.2018.1549100?casa_token=eO-_tpyCd3wAAAAA:w9MZzAbhBb6HM-2iOPyvD2Y2veCac-HA6oDUc2drJEicqA8hXZ5IlzZUbmwtsdCWAbFzPi7kMJX1lw for thoughts on the R2 method.
dirmod1 %>% 
  get_value("rsq") %>%
  plot()

# Summary of ppc
dirmod1 %>%
  get_value("ppc") %>%
  summary() # Violating independence. Others OK.

# Plot results of the ppc
## NOTE: it's best to execute these plots in the console
plot(dirmod1 %>% 
            get_value("ppc"), 
          "normality") %>%
      plot() # Two-sided test ==> violation 
plot(dirmod1 %>% 
            get_value("ppc"), 
          "heteroskedasticity") %>%
      plot() # One-sided test ==> violation (data are more heteroskedastic)
plot(dirmod1 %>%
            get_value("ppc"), 
          "independence") %>%
      plot() # Two-sided test ==> violation (data are muchos correlated)

# Get the average correlation in the residuals
dirmod1 %>%
  get_value("ppc") %>%
  get_value("data") %>%
  .$independence %>%
  apply(2, mean)

# Soooo, the average correlation in the observed data is +-.46

# Save this model
saveRDS(dirmod1, "models/model1.rds")
```

```{r model2, include = FALSE}
# Model 2: Age, gender and sector -----

# Hypothesis: the relationship between gender and compensation differs across sectors
# We have three: financial (base case), services and Basic materials

# Create the model
dirmod2 <- blm("Compensation ~ Sector + Age + Male",
                data=directors) 

# Retrieve parameter names
dirmod2 %>% get_parameter_names()

# Update the model specification
dirmod2 <- dirmod2 %>%
  # Need to delete posterior when we change settings
  # (only has an effect if posterior already present)
  delete_posterior() %>%
  # Set an informative prior on the coefficients
  # Males (b4) are speculated to have 17% pay increase over women
  set_prior("b4", mu = .17, sd=0.01) %>%
  # Change the sampler of Age to metropolis hastings (random walk)
  # Lambda parameter controls the variance of the (normal) proposal distribution
  set_sampler("b3", type="MH", lambda=0.01) %>%
  # Update sampling settings
  set_sampling_options(chains = 2, iterations = 80000,
                       burn = 15000, thinning=5) %>%
  # Set initial values
  # If we draw from the priors the starting values will be too large
  # This is an issue for MH because it takes many more iterations to converge
  set_initial_values(chain_1 = list("b" = c(4, 2, 3, 8, 10), "sigma"= 1),
                     chain_2 = list("b" = c(2, 4, 7, 2, 3), "sigma"=2)) %>%
  # Compute null model
  compute_null_model() %>%
  # Sample the posterior distribution
  sample_posterior()

# Evaluate the accepted draws
dirmod2 %>% 
  evaluate_accepted_draws() # ~ 40-45%

# Look at burn-in diagnostics
dirmod2 %>% 
  evaluate_convergence_diagnostics() # GR: chains have converged

# Check history
plot(dirmod2, "history") # This seems OK
# Check autocorrelation
plot(dirmod2, "autocorrelation") # Little bit, but not unexpected given MH sampler. At any rate, not too big so not really an issue.

# Evaluate the effect of MH on the effective sample size
dirmod2 %>% 
  evaluate_effective_sample_size() # OK

# View densities
plot(dirmod2, "density") # OK

# Summary
summary(dirmod2)
# ==> Male still not a predictor
# ==> age now a predictor
# ==> Both sectors increase compensation for directors

# R-squared value
# Compute a z-score difference from the posterior to the prior
# Idea: how much does the posterior shift when confronted with new data?
#        calculate on the scale of the posterior
thprior <- rnorm(1, mean=dirmod2$priors$b4$mu, sd=dirmod2$priors$b4$sd)
ps <- get_posterior_samples(dirmod2) 
pmu <- apply(ps, 2, mean)
psd <- apply(ps, 2, sd)
z <- abs((pmu["Male"] - thprior) / psd["Male"])
shrink <- psd[5] / .01 # Factor of 6 ==> this means that the posterior variance is 6 times larger than the prior variance.

# Plot
plot(dirmod2$rsq)
median(dirmod2$rsq$rsquared) # R-squared ~ .089
quantile(dirmod2$rsq$rsquared, c(0.025, 0.25, 0.5, 0.75, 0.975)) # 95% CCI [.045, .15]

# Posterior predictive checks
dirmod2 <- dirmod2 %>%
  evaluate_ppc(p=.5)

# Residual plot
errs <- data.frame(
  "residuals_model" = resid(dirmod2),
  "predicted_model" = predict(dirmod2),
  "company" = directors$Company
)

library(ggplot2)
p1 <- ggplot(errs, aes(x=predicted_model, y=residuals_model, color=as.factor(company))) +
  geom_point() +
  theme_blm() +
  theme(legend.position="none")
library(purrr)
rc <- errs %>%
  split(.$company) %>%
  map(function(x) {
    x$residuals_lagged <- c(NA, x$residuals_model[1:length(x$residuals_model)-1])
    cor(x$residuals_model, x$residuals_lagged, use="complete.obs")
  }) %>%
  unlist()

rc <- data.frame(
  "cor" = unname(rc),
  "comp" = names(rc)
)

p2 <- ggplot(rc, aes(x=reorder(comp, -abs(cor)), y=cor)) +
  geom_bar(stat="identity") +
  theme_blm() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank()) # ==> correlation by boards, should do a multilevel model here to accomodate hierarchical structure of the data.

library(gridExtra)
grid.arrange(p1, p2, ncol=2)
# Summary
summary(dirmod2$ppc)
p <- plot(dirmod2$ppc, "normality") ; plot(p) # Two-sided test ==> violation ==> less skewed compared to model 1
p <- plot(dirmod2$ppc, "heteroskedasticity") ; plot(p) # One-sided test ==> violation (data are more heteroskedastic) ==> more extreme than model 1
p <- plot(dirmod2$ppc, "independence") ; plot(p) # Two-sided test ==> violation (data are correlated) ==> less extreme than model 1 so we're better taking into account group effects

## Autocorrelation ==> not strange. Board members of the same COMPANY are more alike than board members of other company. Hence the correlation in the errors.

# Model fit
dirmod2 %>%
  get_value("DIC") %>%
  summary()# ==> DIC is 487.895 with 6 parameters ==> fit is better than model 1

# Save this model
saveRDS(dirmod2, "models/model2.rds")
```

```{r PPC results, eval=FALSE, fig.width=7, fig.height=2, fig.cap="A random subset of observed and simulated correlation coefficients for model 1 (left) and model 2 (right). The figure shows that the observed residuals (in red) are much more correlated than is reasonable under the model (represented by the blue, simulated values)."}
# Collect ppc results
ppc1 <- do.call(cbind.data.frame, dirmod1$ppc$results)
ppc2 <- do.call(cbind.data.frame, dirmod2$ppc$results)

# Bind rows
ppc_results <- round(rbind(ppc1, ppc2), digits=4)
row.names(ppc_results) <- c("Model 1", "Model 2")

# Get values for independence ppc
indeppc1 <- dirmod1 %>% get_value("ppc") %>%
  get_value("data") %>% .$independence %>%
  as.data.frame() %>%
  tidyr::gather(type, value) %>%
  mutate(type = ifelse(type == "V1", "Simulated", "Observed"),
         samp = runif(1:n())) %>%
  filter(samp <= .15) %>%
  group_by(type) %>%
  mutate(index = 1:n()) %>%
  ggplot(aes(x=index, y=value, color=type)) +
    geom_point() +
    theme_blm() +
    scale_color_brewer(palette = "Set1") +
    theme(axis.text.x = element_blank(),
          legend.title = element_text(rel(1.2)),
          legend.text = element_text(rel(1.2)),
          axis.title.y = element_text(size=12),
          axis.title.x = element_blank(),
          legend.position = "none") +
    scale_y_continuous(name = latex2exp::latex2exp("Pearson corr. coef.")) +
  labs(title="(b) Model 2", subtitle = "PPC independence of errors")

# Model 2
indeppc2 <- dirmod2 %>% get_value("ppc") %>%
  get_value("data") %>% .$independence %>%
  as.data.frame() %>%
  tidyr::gather(type, value) %>%
  mutate(type = ifelse(type == "V1", "Simulated", "Observed"),
         samp = runif(1:n())) %>%
  filter(samp <= .15) %>%
  group_by(type) %>%
  mutate(index = 1:n()) %>%
  ggplot(aes(x=index, y=value, color=type)) +
    geom_point() +
    theme_blm() +
    scale_color_brewer(palette = "Set1") +
    theme(axis.text.x = element_blank(),
          axis.title.y = element_blank(),
          legend.title = element_text(rel(1.2)),
          legend.text = element_text(rel(1.2)),
          axis.title.x = element_blank(),
          legend.position = "none") +
  labs(title="(b) Model 2", subtitle = "PPC independence of errors")

gridExtra::grid.arrange(indeppc1, indeppc2, ncol=2)
```

From the posterior predictive checks (table XY), we observe that the data are not independent $(\hat{r}_{\text{PPC, Model1}} = .466)$. That is, directors who are in the same board tend to are more similar to each other than directors from other boards. This is not unexpected: the data are hierarchical in nature such that individuals are nested in boards. Indeed, the results of the random effects model shows that the intra-class correlation coefficient $\rho$ equals $57\%$, meaning that the expected correlation of two randomly picked directors from the same company is $\hat{r}=.573$.^[The results of running all different stages of a multilevel model are presented in another document.] 

```{r MLM intercept-only model}
library(rjags)

# Reload directors
data("directors")

# Preprocess directors data
library(dplyr)
directors <- directors %>%
  # Log compensation
  mutate(Compensation = log(Compensation),
         Male = as.numeric(Male) - 1) %>%
  # Create level 2 variables
  group_by(Company) %>%
  mutate(avgAge = mean(Age),
         propMale = sum(Male) / n(),
         # Group-mean center individual data
         Age = Age - mean(Age),
         Male = Male - mean(Male)) %>%
  # Ungroup
  ungroup() %>%
  # Grand-mean center group variables
  mutate(avgAge = avgAge - mean(avgAge),
         propMale = propMale - mean(propMale)) %>%
  # Sort data (makes it easier later)
  arrange(Sector)
  
# Jags data
dir_jags <- with(directors, list(## Outcome for individuals (level 1)
  
                                 compensation = Compensation,
                                 # Age of individuals
                                 age=Age,
                                 # Gender of individuals
                                 gender=Male,
                                 
                                 ## Company-level variables (level 2)
                                 
                                 # Company indicator
                                 company=as.numeric(as.factor(Company)),
                                 # Average age of directors for each company
                                 avgAge=avgAge,
                                 # Proportion of males for each company
                                 avgMale=propMale,
                                 
                                 ## Group totals
                                 
                                 # Number of individuals
                                 n=nrow(directors),  
                                 # Number of companies
                                 k=length(unique(Company)),
                                 
                                 ## Number of predictors
                                 
                                 # Number of individual-level predictors
                                 p1=2,
                                 # Number of company-level predictors
                                 p2=2))

# Intercept-only model
io_mod <- "model {

  ### Level 2 ==> companies

	# Priors (companies)
	tau_u0 ~ dgamma(.01, .01)
	# Hyperprior for mean
  gamma_00 ~ dnorm(0, 1.0E-4)
  # For each company
	for (j in 1:k) {
	  # For each company, draw from normal
    b0[j] ~ dnorm(gamma_00, tau_u0)
	}

	### Level 1 ==> individuals

  # Priors
  # Individual precision
	tau ~ dgamma(.01, .01)  # standard deviation of fixed effect (variance within companies)
  # For each individual
	for (i in 1:n) {
		compensation[i] ~ dnorm(mu[i], tau) # Combine likelihood and priors
		mu[i] <- b0[company[i]] # Linear combination for each person
	}

  # Invert gamma
  sigma_e <- 1 / sqrt(tau)
  sigma_u0 <- 1 / sqrt(tau_u0)

}"

# Run the model in JAGS
# Initial values
dir_inits <- list(
  init1 <- list(tau=runif(1), tau_u0=runif(1)),
  init2 <- list(tau=runif(1), tau_u0=runif(1))
)

# Specify model in JAGS
mod_io <- jags.model(textConnection(io_mod), 
                     data = dir_jags,
                     inits = dir_inits,
                     n.chains=2)

# Burn
update(mod_io, n.iter=60000)

# Draw samples
params <- c("sigma_e", "sigma_u0", "gamma_00")
# Run the chain
resm1 <- coda.samples(mod_io, variable.names = params, n.iter=500000, thin = 10)

# Convergence?
#plot(resm1)

# DIC
DICm1 <- dic.samples(mod_io, thin=5, n.iter=20000, type="pD") # 288.6

# MAP values
MAPm1 <- apply(do.call(rbind.data.frame, resm1), 2, mean)

# Coerce to matrix
resmat <- do.call(rbind, resm1)

# New matrix with same dims
ve <- matrix(0, ncol=1, nrow=nrow(resmat))

# FOr each row
# Posterior distribution of variance explained
for(i in seq_along(1:nrow(resmat))) {
  ve[i,] <- c(resmat[i,3]^2 / sum(resmat[i,-1]^2))
}

quantile(ve, c(0.025, .25, .5, .75, .975))
# VE between .45 and .69

# Save the model and DIC
#saveRDS(resm1, "models/ML_interceptonly.rds")
#saveRDS(DICm1, "models/ML_io_DIC.rds")
```

The final random effects model is presented in table YY. This model corresponds to the following equation.

$$
\tag{3}
\begin{aligned}
\text{compensation}_{ij} =
&\gamma_{00} + u_{0j} + \gamma_{10}\cdot \text{Age}_{ij} + e_{ij}
\end{aligned}
$$

Where $\gamma_{00}$ is the overall intercept and $u_{0j}$ is a company-specific error term. Notice that the fit of this model is much better than that of the previous models, indicating that the multilevel approach seems appropriate. The marginal and conditional R-squared values [CITE] are $R^2_{\text{M}} = .0056; \ 95\% \ \text{CCI}=[.0008, .015]$ and $R^2_{\text{C}} = .5398; \ 95\% \ \text{CCI}=[.4807, .6]$ respectively. This indicates that the fixed part of the model (age) explains almost no variation in the data, but the fixed and random parts together explain some $54\%$ of the total variation. Hence, we must conclude that we do not have the right variables at either level 1 or 2 that would help us explain the heterogeneity in comepnsation among directors.

```{r MLM level-1 predictors}
# Initial values
dir_inits <- list(
  init1 <- list(tau=runif(1), tau_u0=runif(1)),
  init2 <- list(tau=runif(1), tau_u0=runif(1))
)

# Model
mlm_2 <- "model {

  ### Level 2 ==> companies

	# Priors (companies)
	tau_u0 ~ dgamma(.01, .01)
	# Hyperprior for mean
  gamma_00 ~ dnorm(0, 1.0E-4)
  gamma_10 ~ dnorm(0, 1.0E-4)
  # For each company
	for (j in 1:k) {
	  # For each company, draw from normal
    b0[j] ~ dnorm(gamma_00, tau_u0)
	}

	### Level 1 ==> individuals

  # Priors
  # Individual precision
	tau ~ dgamma(.01, .01)  # standard deviation of fixed effect
  # For each individual
	for (i in 1:n) {
		compensation[i] ~ dnorm(mu[i], tau) # Combine likelihood and priors
		mu[i] <- b0[company[i]] + # Intercept
		         gamma_10*age[i] #+ gamma_20*gender[i] # Level 1 variables
	}

  # Invert gamma
  sigma_e <- 1 / sqrt(tau)
  sigma_u0 <- 1 / sqrt(tau_u0)

}"

# Specify model in JAGS
mod_io <- jags.model(textConnection(mlm_2), 
                     data = dir_jags,
                     inits = dir_inits,
                     n.chains=2)

# Burn
update(mod_io, n.iter=60000)

# Draw samples
params <- c("sigma_e", "sigma_u0", "gamma_00", "gamma_10")
# Run the chain
resm2 <- coda.samples(mod_io, variable.names = params, n.iter=500000, thin = 10)

# DIC
DICm2 <- dic.samples(mod_io, thin=5, n.iter=20000, type="pD") # 280.9 => +- 9 difference

# Calculate variance explained
# MAP values for model 1
MAPm1 <- apply(do.call(rbind.data.frame, resm1), 2, mean)
MAPm2 <- apply(do.call(rbind.data.frame, resm2), 2, mean)

# Map values
#MAPve <- apply(ve_2, 2, median)

# Save model
saveRDS(DICm2, "models/ML_final_DIC.rds")
saveRDS(resm2, "models/model3.rds")
```

```{r calculate variance explained for mixed model, eval=TRUE}
# Calculate R^2. Marginal (only explained by predictors) & Conditional (predicted by variance component)
# See: https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210x.2012.00261.x

# Function to calculate marginal & conditional R^2
# @param X design matrix containing fixed part of the model
# @param w coefficient matrix
# @e_1 random part of level 1
# @e_2 random part of level 2
MLM_R2 <- function(X, w, e_1, e_2) {
  
  # Component 1: fixed effect
  e_f <- var(X %*% w)

  # Compute marginal & conditional R^2
  mar <- (e_f) / (e_f + e_1 + e_2) # Variance explained by fixed factors (marginal R^2)
  con <- (e_f + e_2) / (e_f + e_1 + e_2) # Variance explained by fixed and random factors (conditional R^2)
  
  # Return
  return(
    c(mar, con)
  )
  
}

# Coerce to matrix
resmat <- do.call(rbind, resm2)

# New matrix with same dims
ve_2 <- matrix(0, ncol=2, nrow=nrow(resmat))

# X for fixed effect
X <- matrix(directors$Age, ncol=1)

# FOr each row
# Posterior distribution of variance explained (pseudo R-squared)
for(i in seq_along(1:nrow(resmat))) {
  ve_2[i,] <- MLM_R2(X, 
                 resmat[i,2],
                 resmat[i,3],
                 resmat[i,4])
}

# Median var explained
mve <- apply(ve_2, 2, median)
```

\begin{table}[H] \centering 
\begin{small}
  \caption{Model results } 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{3}{c}{Compensation (GBR '000, logged)} \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\\[-1.8ex] & Linear & Linear & Linear mixed effects \\
\\[-1.8ex] & (blm) & (blm) & (JAGS) \\
\hline \\[-1.8ex] 
\textit{(a) Fixed} & & & \\
Constant & 4.991  (4.936, 5.046) & 4.821 (4.732, 4.910) & 4.991 (4.877, 5.110) \\ 
SectorBasic Materials &  & .225 (.079, .370) &  \\ 
SectorServices &  & .292 (.169, .414) &  \\ 
Male & .065 ($-$.085, .215) & .153 (.095, .209) & \\ 
Age & .008 (0.000, .017) & .009 (.001, .017) & .001 (.0035, .0155) \\ 
\hline \\[-1.8ex] 
\textit{(b) Random} & & & \\
$\sigma^2_e$ & .516 & .501 & .342 \\
$\sigma^2_{u0}$ & & & .399\\
\hline \\[-1.8ex] 
\textit{(c) Model Fit} \\
Observations & 336 & 336 & 336 \\
Companies & & & 52 \\
DIC & 508 & 488 & 280 \\ 
Penalty. & 4 & 6 & 47 \\ 
R$^{2}$ & .021 & .098 & .006 (M), .536 (C) \\ 
BF & .112 & 37.596 & \\
\hline \\[-1.8ex] 
\textit{(d) Post. Pred. Checks} & & & \\
Normality & .361 & .353 &  \\
Homoskedasticity & .326 & .607 & \\
Independence & 0 & 0 & \\
\hline \\[-1.8ex] 
\textit{(e) Bayes' Factors} & & & \\
$\text{H}_1$ & 4.5 & 5.2 &  \\
$\text{H}_2$ & 3.4 & 6 & \\
$\text{H}_2$ & .3 & .2 & \\
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{r}{Baseline is sector 'Financials' for models (1) and (2)} \\
\end{tabular} 
\end{small}
\end{table}

```{r classical residuals}
plot(predict(dirmod2), resid(dirmod2))
```



```{r r-squared plot, fig.cap="Bayesian R-squared value for model 1 (a) and model 2 (b). The proportion of cases in which the R-squared value of model 2 exceeds that of model 1 is .99. Figure (c) shows the marginal (blue) and conditional (red) R-squared values for the posterior distribution. The marginal R-squared indicates the amount of variance explained by the fixed part of the model; the conditional R-squared indicates the amount of variance explained by the fixed and random part of the model.", fig.height=2, fig.width=7, eval=TRUE}
library(latex2exp)
library(ggplot2)
# Plot r-squared
p1 <- dirmod1 %>% get_value("rsq") %>% plot() + theme_blm(text_size = 8) +
  theme(axis.title = element_blank(),
        axis.text.y = element_blank(),
        plot.subtitle = element_blank()) +
  labs(title = latex2exp::TeX("\\textbf{(a) Bayesian $R^2$ posterior (model 1)}"))
# Model 2
p2 <- dirmod2 %>% get_value("rsq") %>% plot() + theme_blm(text_size = 8) +
  theme(axis.title = element_blank(),
        axis.text.y = element_blank(),
        plot.subtitle = element_blank()) +
  labs(title = latex2exp::TeX("\\textbf{(b) Bayesian $R^2$ posterior (model 2)}"))
# Model 3 (multilevel)
veg <- ve_2 %>%
  as.data.frame() %>%
  rename("MR2" = V1, "CR2" = V2) %>%
  #select(CR2) %>%
  tidyr::gather(type, rsq)
# Plot
p3 <-  ggplot(veg, aes(x=rsq, fill=type)) +
    geom_histogram(alpha=.4, color="black") +
    theme_blm(text_size=8) +
    scale_fill_brewer(palette="Set1") +
    geom_vline(xintercept=veg %>% filter(type=="MR2") %>% 
                 select(rsq) %>% pull() %>% 
                 median(), linetype="dashed") +
    geom_vline(xintercept=veg %>% filter(type=="CR2") %>% 
                 select(rsq) %>% pull() %>% 
                 median(), linetype="dashed") +
    theme(axis.title = element_blank(),
          axis.text.y = element_blank(),
          legend.position = "none") +
    labs(title = latex2exp::TeX("\\textbf{(c) Marginal and Conditional $R^2$ posterior}"))

# Grid
grid.arrange(p1, p2, p3, ncol=3)
```

## Other material

If we desire to interpret this p-value like we do the traditional, Fisherian p-value, then we must assume that, under the assumption that a particular test is not violated, the Bayesian p-value is uniformly distributed. Otherwise, we cannot interpret it as a proportion. Hence, we desire $p_{\text{posterior}} = P(x \leq X)$. We often find that this assumption does not hold true for posterior predictive checks. To this end, we can simulate the assumption when it holds [CHANGE]. This is illustrated in figure XX for the posterior predictive checks included in the R library blm.

```{r ppc simulations, fig.cap="Distributions of posterior predictive p-values for 1.000 simulated data sets. In plots (a) and (b), the simulated data are drawn from a normal without any violations of the linear regression assumptions. In plot (c), the assumption of homoskedasticity is violated in each of the simulations. In plot (d), the assumption of independence of errors is violated in each of the simulations. The color indicates the severity of the violation; the green bars indicate mild violation, blue indicates medium violation and red indicates severe violation. The script used to generate the data and run the simulations may be found here.", fig.width=7, fig.height=3, eval=TRUE}

# Convenience function to flatten array
flatten_array <- function(x) {
  
  # To list and return
  lapply(1:dim(x)[3], function(y) {
    ss <- x[,,y]
    colnames(ss) <- c("normality", "heteroskedasticity", "independence")
    # To df
    ss <- as.data.frame(ss)
    # Return
    return(ss)
  })
  
}

# Read data
normal <- readRDS("data/simulations/ppc_normal_data.rds")
finsim <- readRDS("data/simulations/final.rds")
heterosked <- finsim$heterosked$results %>% 
  flatten_array() %>%
  bind_rows(.id="severity") %>%
  mutate(severity = factor(severity, labels = finsim$heterosked$degrees %>% names()))
indep <- finsim$indep$results %>% 
  flatten_array() %>%
  bind_rows(.id="severity") %>%
  mutate(severity = factor(severity, labels = finsim$heterosked$degrees %>% names()))

# PPC heteroskedasticity, no violation
p1 <- ggplot(data.frame(x=normal[,2]), aes(x=x)) +
  geom_histogram(color="black", fill="lightgrey") +
  scale_x_continuous(limits=c(0,1)) + 
  theme_blm(text_size = 10) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_blank()) +
  labs(title = "(a) PPC heteroskedasticity", subtitle="      no violations")
# PPC independence, no violation
p2 <- ggplot(data.frame(x=normal[,3]), aes(x=x)) +
  geom_histogram(color="black", fill="lightgrey") +
  scale_x_continuous(limits=c(0,1)) + 
  theme_blm(text_size = 10) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_blank()) +
  labs(title = "(b) PPC independence", subtitle="      no violations")
# PPC heteroskedasticity, assumption violated
p3 <- ggplot(heterosked, aes(x=heteroskedasticity, fill=severity)) +
  geom_histogram(color="black", alpha=0.4, position = "identity")  + 
  theme_blm(text_size = 10) + 
  scale_fill_brewer(palette = "Set1", direction=-1) +
  scale_x_continuous(limits=c(-0.02,1)) +
  theme(axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none") +
  labs(title = "(c) PPC heteroskedasticity", subtitle="      assumption violated")
# PPC independence, violated
p4 <- ggplot(indep, aes(x=independence, fill=severity)) +
  geom_histogram(color="black", alpha=0.4, position = "identity")  + 
  scale_fill_brewer(palette = "Set1", direction=-1) +
  scale_x_continuous(limits=c(-0.02,1)) +
  theme_blm(text_size = 10) +
    theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none") +
  labs(title = "(d) PPC independence", subtitle="      assumption violated")

# Merge plots
grid.arrange(p1, p2, p3, p4)
```

# References

Lynch, S. M. (2007). Introduction to applied Bayesian statistics and estimation for social scientists. Springer Science & Business Media. Chapter 9.2

Aarts, E. (2019). Introduction to multilevel analysis and the basic two-level regression model, week 1 notes [powerpoint presentation]. *Introduction to Multilevel Analysis*, Utrecht University.

Nakagawa, S., & Schielzeth, H. (2013). A general and simple method for obtaining R2 from generalized linear mixed‐effects models. Methods in Ecology and Evolution, 4(2), 133-142.
