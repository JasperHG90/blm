---
title: 'The Gender Gap in Independent Director Compensation'
author: "Jasper Ginn (s6100848)"
date: "April 8, 2019"
output:
  pdf_document:
    number_sections: false
bibliography: bib.bib
geometry: margin=2.5cm
header-includes:
 \usepackage{float}
 \restylefloat{table}
 \usepackage{setspace}
 \usepackage{xcolor}
 \usepackage[font={small}]{caption}
 \singlespacing
 \floatplacement{figure}{H}
---
<!-- Set font size -->
\fontsize{11}{22}

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos="H", fig.width = 7, fig.height = 4, echo = FALSE, 
                      fig.align="center", warning = FALSE, message = FALSE, eval=FALSE)

rm(list=ls())

# Load blm package
library(blm)

# Load ggplot
library(ggplot2)
library(gridExtra)
library(dplyr)

# Get the directors data
data("directors")

# remove where outcome / covariate age is NA
# (blm cannot handle missing values)
directors_with_na <- directors
directors <- directors %>%
  filter(!is.na(Age),
         !is.na(Compensation))

# Keep for later
dirs <- directors

# Preprocess directors data
# This is a subset of the original (proprietary) dataset
# 5 sectors have not been included due to bimodality in the data. 
# See the folder 'data-raw' for the script used to create the directors data.
library(dplyr)
directors <- directors %>%
  # Log compensation
  mutate(Compensation = log(Compensation)) %>%
  # Subtract mean from Age variable
  mutate(Age = Age - mean(Age),
         Male = as.numeric(Male) - mean(as.numeric(Male)))

# Load models 
# i.e. prevent R from re-running the models the entire time when knitting the doc (this takes a long time).
# Instead, run code line-by-line if you need to.
dirmod2 <- readRDS("models/model2.rds")
resm2 <- readRDS("models/model3.rds")
```

```{r julia setup}
# Set up blm julia
blm_setup() 
```

# Introduction

The gender gap in compensation remains a pervasive problem despite efforts to achieve income parity between men and women. In this paper, I examine the gender gap in compensation among $336$ independent directors in three sectors and $52$ companies using Bayesian Linear Regression. The boards of American companies consists of *executive* and *independent* directors. Execute directors are employed by the firm and usually have high managerial positions within the company, while independent directors are concerned primarily with oversight. 

The results indicate that we cannot conclude that male independent directors receive a higher remuneration than female independent directors after controlling for age. Model fit statistics and evaluation suggest that linear regression is not an appropriate method to analyse these data because director compensation is highly correlated within companies.  

The paper is structured as follows. Section one elaborates on the differences between Frequentist and Bayesian statistical inference. In turn, section two outlines the hypotheses, model specification and reports the results of the analyses. Section three concludes.

# Differences in Bayesian and Frequentist Inference

Many differences between Frequentist and Bayesian inference originate in their respective view of uncertainty, how this is captured by probabilities and what causes this uncertainty. In the Frequentist framework, a probability is defined as the number of times an event will occur in the long run. In other words, it is the limiting value of $m$ successes in a sequence of $n$ trials for a particular event, or $p = \lim_{n \to \infty} \frac{m}{n}$. [@miller2014john, p.21].

This definition has two implications. The first is that talking about probability makes sense only in the context of infinite trials. The second is that probability converges to some fixed quantity as the number of trials go to infinity, and our uncertainty about the true value of this quantity reduces as we repeatedly take more (or larger) samples. In turn, this implies that the only source of randomness by which our estimate $\hat{p}$ differs from the true value $p$ comes from the data, which may differ from sample to sample due to, for example, sampling error. [@miller2014john]

The Bayesian framework considers probabilities as a means of quantifying uncertainty about knowledge [@gelman2013bayesian, pp. 11-13]. Even though the 'true' parameter value may be fixed, we are limited by our knowledge of this value. The uncertainty in our knowledge will hopefully (but not necessarily) decrease as we collect more information and is represented by the posterior distribution.

### Objective and Subjective Knowledge

A Frequentist believes that evidence can only originate from the data. This is not the case in Bayesian statistical inference, where inferences are based on a mix of domain expertise (*prior* or *belief*) and evidence from the data. This makes perfect sense in the Bayesian framework; if we are certain about our knowledge then we can constrain the parameter space by injecting what we know *a priori*. In other words, a Bayesian looks upon prior beliefs as just another source of knowledge that have been translated into a probability density. 

The claim that the Frequentist approach is more objective only holds if one regards the data collection process, (pre-)processing steps and analysis as guaranteed to be objective. This is a tenuous assumption at best in the social sciences; statistics is fraught with subjective decisions during data collection, manipulation and analysis [@berger1988statistical], and the charge of subjectivity leveled at Bayesians boils down to the practice of incorporating domain knowledge *explicitly* through the use of a prior distribution. 

### Methods of Estimation and Hypothesis Testing

Whether a statistician regards the data or the parameters as a random variable determines their choice of estimation method. A Frequentist will want to find the most likely combination of parameters $\hat{\theta}$ that explain the data and that provide consistent and asymptotically unbiased estimators. Conversely, given that a Bayesian thinks of the data as fixed and the parameters as random variables, they are interested in finding the distribution of the parameters and hence the source of uncertainty in our beliefs after seeing the evidence [@miller2014john, ch.8]. The different estimation methods provide a different type of prediction under the model; whereas Maximum Likelihood (ML) used by Frequentists estimates yield point estimates, MCMC methods employed by Bayesians yield posterior probability distributions. The latter can be used to evaluate, for example, the fit of the model by means of Posterior Predictive Checks (PPP).

<!--
Bayesian models yield the posterior probability distribution of the model parameters. This allows us to produce a posterior predictive distribution for each subject in the data set. Unlike Frequentist model results, such distributions  provide us with a useful way to understand whether the model adheres to its core assumptions by means of *posterior predictive checks* (PPC). PPC require us to simulate data under the model and to compare the simulated outcome variable to the observed outcome variable, which results in a proportion sometimes called the Bayesian p-value. If we desire to interpret this p-value like we do the traditional, Fisherian p-value, then we must assume that, under the assumption that a particular test is not violated, the Bayesian p-value is uniformly distributed. We often find that this assumption does not hold true for posterior predictive checks. To this end, we can simulate data under various conditions to observe how the statistic behaves. This is illustrated in figure 1 below for two posterior predictive checks included in the R library blm.
-->

```{r simulation script}
## Simulation study for PPC
## Plan: Run the plans below ==> without any violations and violating to different degrees

library(blm)
blm_setup()

# iterations
k <- 1000

# Results
sim_res <- array(0L, c(k, 3, 3))

# Grid of values to test
# See: generate_dataset in helpers.R (line 222)
grid <- list(
  "noviolation" = list(
    "results" = array(0L, c(k, 3, 1)),
    "degrees" = 1 # Degrees will be ignored if no violation but it eases coding in the for-loop below
  ),
  "heterosked" = list(
    "degrees" = c("mild" = 1, "medium"=3, "severe" = 5),
    "results" = sim_res
  ),
  "indep" = list(
    "degrees" = c("mild"=0.1, "medium"=0.5, "severe"=0.9), # Determines 'ar' param for an arima.sim
    "results" = sim_res
  )
)

# For each test
# Progress bar
library(utils)
pb <- txtProgressBar(min = 0, max = (k * (3 * 2)) + k, style = 3) # (k * (length(grid) * length(degrees))) + k [==>] (last k is for no violation)
# keep track of total i for progress bar
i_pb <- 0
# For each assumption
for(j in seq_along(grid)) {

  # For each degree
  for(d in seq_along(grid[[j]][["degrees"]])) {

    # For each simulation
    for(i in 1:k) {

      # Increment i_pb
      i_pb <- i_pb + 1

      # Set pb
      setTxtProgressBar(pb, i_pb)

      # Unroll data
      noviolation <- ifelse(j == 1, TRUE, FALSE)
      heteroskedastic <- ifelse(j == 2, TRUE, FALSE)
      independence <- !heteroskedastic & !noviolation

      # Simulate data
      dat <-blm::blmsim(n = 100, j=2, binary = 0, heteroskedastic = heteroskedastic,
                        correlated_errors = independence, degree=grid[[j]][["degrees"]][[d]])

      # Unroll
      df <- as.data.frame(cbind(dat$y, dat$X[,-1]))
      # Names
      colnames(df) <- c("y", paste0("x", 1:(ncol(df) - 1)))

      # Blm object
      bfit <- blm("y ~ .", data=df) %>%
        set_sampling_options(iterations = 15000, burn = 2000, chains = 1) %>%
        sample_posterior() %>%
        # PPC
        evaluate_ppc(p=1)

      # Store ppc
      ppcr <- get_value(bfit, "ppc")
      # Store results
      grid[[j]][["results"]][i, , d] <- c(ppcr$results$normality, ppcr$results$homosked, ppcr$results$independence)

      # Save every 10 iterations
      if(i %% 10 == 0) {
        # Store data
        saveRDS(grid, "experiments/2_Simulations_PPC/ppc_simulated/tmp.rds")
      }

    }

  }

}

# Store data
saveRDS(grid, "experiments/2_Simulations_PPC/ppc_simulated/final.rds")
```

```{r ppc simulations, fig.cap="Distributions of posterior predictive p-values for 1.000 simulated data sets. In plots (a) and (b), the simulated data are drawn from a normal without any violations of the linear regression assumptions. In plot (c), the assumption of homoskedasticity is violated in each of the simulations. In plot (d), the assumption of independence of errors is violated in each of the simulations. The color indicates the severity of the violation; the green bars indicate mild violation, blue indicates medium violation and red indicates severe violation.", fig.width=7, fig.height=3, eval=TRUE}

# Convenience function to flatten array
flatten_array <- function(x) {
  
  # To list and return
  lapply(1:dim(x)[3], function(y) {
    ss <- x[,,y]
    colnames(ss) <- c("normality", "heteroskedasticity", "independence")
    # To df
    ss <- as.data.frame(ss)
    # Return
    return(ss)
  })
  
}

# Read data
normal <- readRDS("data/simulations/ppc_normal_data.rds")
finsim <- readRDS("data/simulations/final.rds")

normal <- finsim$noviolation$results %>%
  flatten_array() %>%
  bind_rows()
heterosked <- finsim$heterosked$results %>% 
  flatten_array() %>%
  bind_rows(.id="severity") %>%
  mutate(severity = factor(severity, labels = finsim$heterosked$degrees %>% names()))
indep <- finsim$indep$results %>% 
  flatten_array() %>%
  bind_rows(.id="severity") %>%
  mutate(severity = factor(severity, labels = finsim$heterosked$degrees %>% names()))

# PPC heteroskedasticity, no violation
p1 <- ggplot(data.frame(x=normal[,2]), aes(x=x)) +
  geom_histogram(color="black", fill="lightgrey") +
  scale_x_continuous(limits=c(0,1)) + 
  theme_blm(text_size = 10) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_blank()) +
  labs(title = "(a) PPC heteroskedasticity", subtitle="      no violations")
# PPC independence, no violation
p2 <- ggplot(data.frame(x=normal[,3]), aes(x=x)) +
  geom_histogram(color="black", fill="lightgrey") +
  scale_x_continuous(limits=c(0,1)) + 
  theme_blm(text_size = 10) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_blank()) +
  labs(title = "(b) PPC independence", subtitle="      no violations")
# PPC heteroskedasticity, assumption violated
p3 <- ggplot(heterosked, aes(x=heteroskedasticity, fill=severity)) +
  geom_histogram(color="black", alpha=0.4, position = "identity")  + 
  theme_blm(text_size = 10) + 
  scale_fill_brewer(palette = "Set1", direction=-1) +
  scale_x_continuous(limits=c(-0.02,1)) +
  theme(axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.text.y = element_blank(),
      legend.position = "none") +
  labs(title = "(c) PPC heteroskedasticity", subtitle="      assumption violated")
# PPC independence, violated
p4 <- ggplot(indep, aes(x=independence, fill=severity)) +
  geom_histogram(color="black", alpha=0.4, position = "identity")  + 
  scale_fill_brewer(palette = "Set1", direction=-1) +
  scale_x_continuous(limits=c(-0.02,1)) +
  theme_blm(text_size = 10) +
    theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none") +
  labs(title = "(d) PPC independence", subtitle="      assumption violated")

# Merge plots
grid.arrange(p1, p2, p3, p4)
```

The way in which we think about statistical inference under these frameworks is illustrated by the difference in interpretation of the confidence interval and the credible interval. When we calculate the confidence interval, the upper and lower confidence limited should be interpreted as random variables because they vary across samples. Crucially, they do not reflect probabilities but confidence in our point estimates [@miller2014john, pp.317-320]. A credible interval, being associated with a posterior distribution, necessarily deals with probability statements such that we can say that the parameter is contained in the credible interval with some probability (e.g. $95\%$) [@lynch2007introduction, p.58].

Further implications are to be found in the way we test hypotheses in these frameworks. In the Frequentist framework, we usually partition the parameter space into an acceptance and a rejection region based on some null and alternative hypothesis. On the basis of a test statistic, computed from the data, we then decide whether or not the result we observe is likely to occur due to chance. We use either confidence intervals or $p$-values to express support for the alternative hypothesis, but, importantly, we can never quantify support in favor of the null hypothesis [@miller2014john, pp.337-339]. This is not so in the Bayesian Framework. Given that the data can be viewed as a means to update a prior belief, hypothesis testing using Bayes Factors is a relative statement about the degree to which the evidence found in the data supports one hypothesis over another [@hoijtink2019tutoial]. 

<!--
TYPE I & II errors, advantages of bayes factor, not effect sizes
-->

# Methods & Results

This section describes the results of the statistical analysis. The outcome variable compensation is given in thousands of Great British pounds and has been log-transformed. The variable age has been grand-mean centered to facilitate interpretation and estimation. Some $44\%$ of the data has been deleted due to missingness on one of the variables, which leaves us with $336$ observations, $60$ of whom are female and $276$ are male.  The model is given by the following equation:

$$
\begin{aligned}
\widehat{\log(\text{compensation}_i)} = 
&\beta_0 + \beta_{1} [\text{age}_i - \overline{\text{age}}] + \beta_2\text{male}_i + \epsilon_i
\end{aligned}
$$

Given that we measure the compensation on a log scale, the coefficients we derive from the model must be interpreted as change in percentages. This forces us to rethink the priors we specify. We assume that, as age increases, compensation increases as well. Given that we are not certain about the extent to which it increases, we set the the mean to $.05$ and the standard deviation of this estimate to $.2$, representing a spread of approximately $20\%$. We know from earlier studies that male executive directors earn $8-25\%$ more than female directors [@bell2005women]. Due to the lack of research with respect to the remuneration of independent directors, we will take this as a baseline and set this prior a mean of $.165$ with a variance of $.1$, which reflects our uncertainty of the prior knowledge. 

In accordance with our expectations, we hypothesize that male directors earn some $5-20\%$ more than their female counterparts ($H_1$) and that compensation only increases with age ($H_2$). $H_u$ is the unconstrained hypothesis.

$$
\text{H}_1\text{: } .05 < \beta_{\text{Male}} < .20 \ \ \ \ \ \ \ \text{H}_2\text{: } \beta_{\text{Age}} > 0 \ \ \ \ \ \ \ \text{H}_u\text{: } \beta_\text{Male}, \beta_{\text{Age}}
$$

```{r model1 and 2, include = FALSE}
# Model 2: Age and Gender -----

# Create the model
dirmod2 <- blm("Compensation ~ Age + Male",
                data=directors) 

# Retrieve parameter names
dirmod2 %>% get_parameter_names()

# Update the model specification
dirmod2 <- dirmod2 %>%
  # Need to delete posterior when we change settings
  # (only has an effect if posterior already present)
  delete_posterior() %>%
  
  # Change the uninformative priors for the intercept and the sectors
  # We are reasoning in percentage change now. So uninformative priors should reflect this
  # on the same scale. (except the intercept)
  set_prior("b0", mu = 0, sd = 100) %>%
  # Set an informative prior on the coefficients
  # Age centered at 0 with .2 variance
  set_prior("b1", mu = .05, sd = .2) %>%
  # Males (b2) are speculated to have 17% pay increase over women
  set_prior("b2", mu = .165, sd=.1) %>%
  
  # Change the sampler of Age to metropolis hastings (random walk)
  # Lambda parameter controls the variance of the (normal) proposal distribution
  set_sampler("b1", type="MH", lambda=0.01) %>%
  # Update sampling settings
  set_sampling_options(chains = 2, iterations = 80000,
                       burn = 15000, thinning=5) %>%
  
  # Set initial values
  # If we draw from the priors the starting values will be too large
  # This is an issue for MH because it takes many more iterations to converge
  set_initial_values(chain_1 = list("b" = runif(3, 0, 10), "sigma"=1),
                     chain_2 = list("b" = runif(3, 0, 10), "sigma"=2)) %>%
  
  # Compute null model (the intercept-only model)
  compute_null_model() %>%
  
  # Set hypotheses
  set_hypothesis("H1", "b2 > 0.05 & b2 < .2") %>% # Gender: larger than 0
  set_hypothesis("H2", "b1 > 0") %>%   # Age: larger than 0
  
  # Sample the posterior distribution
  sample_posterior()

## Convergence checks

# Print model specification
print(dirmod2)

# Evaluate the accepted draws
dirmod2 %>% 
  evaluate_accepted_draws() # ~ 40-45%

# Look at burn-in diagnostics
dirmod2 %>% 
  evaluate_convergence_diagnostics() # GR: chains have converged

# Check history
plot(dirmod2, "history") # This seems OK
# Check autocorrelation
plot(dirmod2, "autocorrelation") # OK

# Evaluate the effect of MH on the effective sample size
dirmod2 %>% 
  evaluate_effective_sample_size() # OK

# View densities
plot(dirmod2, "density") # OK

# Did the null model converge?
plot(dirmod2, "nullmodel") # OK

## Summary of results

# Summary
summary(dirmod2)
# ==> Male not a predictor
# ==> age not a predictor
# ==> Both sectors increase compensation for directors

# R-squared value
# Plot
plot(dirmod2$rsq)
median(dirmod2$rsq$rsquared) # R-squared ~ .089
quantile(dirmod2$rsq$rsquared, c(0.025, 0.25, 0.5, 0.75, 0.975)) # 95% CCI [.045, .15]

## Posterior predictive checks

# Posterior predictive checks
dirmod2 <- dirmod2 %>%
  evaluate_ppc(p=1)
dirmod2 %>% get_value("ppc")

# Residual plot
errs <- data.frame(
  "residuals_model" = resid(dirmod2),
  "predicted_model" = predict(dirmod2),
  "company" = as.character(directors$Company[!is.na(directors$Compensation)])
)

library(ggplot2)
# Color residuals by company
p1 <- ggplot(errs, aes(x=predicted_model, y=residuals_model, 
                       color=as.factor(company))) +
  geom_point() +
  theme_blm() +
  theme(legend.position="none")
library(purrr)
# Correlation within each board
rc <- errs %>%
  split(.$company) %>%
  map(function(x) {
    x$residuals_lagged <- c(NA, x$residuals_model[1:length(x$residuals_model)-1])
    cor(x$residuals_model, x$residuals_lagged, use="complete.obs")
  }) %>%
  unlist()

rc <- data.frame(
  "cor" = unname(rc),
  "comp" = names(rc)
)
# Plot correlation within each board
p2 <- ggplot(rc, aes(x=reorder(comp, -abs(cor)), y=cor)) +
  geom_bar(stat="identity") +
  theme_blm() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank()) 
# ==> correlation by boards, should do a multilevel model here to accomodate hierarchical structure of the data.

library(gridExtra)
grid.arrange(p1, p2, ncol=2)
# Summary
summary(dirmod2$ppc) # Violating independence assumption
p <- plot(dirmod2$ppc, "normality") ; plot(p) # Two-sided test ==> violation ==> less skewed compared to model 1
p <- plot(dirmod2$ppc, "heteroskedasticity") ; plot(p) # One-sided test ==> violation (data are more heteroskedastic) ==> more extreme than model 1
p <- plot(dirmod2$ppc, "independence") ; plot(p) # Two-sided test ==> violation (data are correlated) ==> less extreme than model 1 so we're better taking into account group effects

## Autocorrelation ==> not strange. Board members of the same COMPANY are more alike than board members of other company. Hence the correlation in the errors.

# Model fit
dirmod2 %>%
  get_value("DIC") %>%
  summary()# ==> DIC is 508 with 4 parameters 
dirmod2 %>%
  get_value("null_model") %>%
  get_value("DIC") %>%
  summary() # ==> 513 so the model above is not really a much better fit.

# Save this model
saveRDS(dirmod2, "models/model2.rds")
```

The model results are presented in table 1. The convergence diagnostics show that the posterior distributions have forgotten their initial states and have converged to their stable distributions. There is no trace of autocorrelation and the Gelman-Rubin statistic is close to $1$, indicating that the chains have converged to the same stationary distribution. The DIC for the model of interest (model 2) is $508$ compared to $513$ for the intercept-only model (model 1). The difference between the DIC scores indicates that the model fits better than the intercept-only model, but only marginally. The MC error, which should be less than $5\%$ of the standard deviation, is negligible.

The model results indicate that gender is not a predictor of director compensation $(\beta_{\text{male}} = .067; 95\% \ \ \text{CCI} = [-.08, .214])$. The CCI and standard deviation for this coefficient indicate that there is a lot of uncertainty in this estimate. Age is also not a predictor and has a small, positive effect on director compensation $(\beta_{\text{age}} = .008; 95\% \ \ \text{CCI} = [-.00, .017])$. This small value may be explained by the fact that the average director in the sample is around $62$ years old and age (serving as a proxy for experience) matters less for compensation. The median $R^2$ value for this model is $R^2 = .021; 95\% \ \ \text{CCI} = [.002, .059])$ [@gelman2018r], indicating that we explain some $2\%$ of the variance in compensation.  

The support for in the data $H_1$ versus $H_c$^[$H_c$ is the complement of $H_1$. Given that we only have inequality constraints, $H_1$ is evaluated against $H_c$ and not $H_u$. See [@hoijtink2019tutoial]] is very small ($BF=1.243$). The evidence that age has a positive impact on compensation ($H_2$) has more support after observing the data ($BF=24.9$), and $H_2$ has $20$ times more support in the data than $H_1$. The posterior model probabilities (PMPb in table 1) indicate that we should prefer $H_2$ over $H_1$. However, this preference comes with a high error probability (approximately $55\%$).

The posterior predictive checks (table 1) suggest that the data violate the independence assumption of the model $(p_{\text{independence}} = 0)$. This is not unexpected: the data are hierarchical in nature such that individuals are nested in boards and directors tend to are more similar to their co-directors in the same boards in terms of compensation. 

```{r MLM intercept-only model}
library(rjags)
library(blm)
library(dplyr)

# Reload directors
data("directors")
# Remove missing values
directors <- directors %>%
  filter(!is.na(Age),
         !is.na(Compensation)) %>%
  # Refactor
  mutate(Company = factor(as.character(Company)))

# Preprocess directors data
library(dplyr)
directors <- directors %>%
  # Log compensation
  mutate(Compensation = log(Compensation),
         Male = as.numeric(Male) - 1) %>%
  # Create level 2 variables
  group_by(Company) %>%
  mutate(avgAge = mean(Age, na.rm=TRUE),
         propMale = sum(Male) / n(),
         # Group-mean center individual data
         Age = Age - mean(Age),
         Male = Male - mean(Male)) %>%
  # Ungroup
  ungroup() %>%
  # Grand-mean center group variables
  mutate(avgAge = avgAge - mean(avgAge),
         propMale = propMale - mean(propMale)) %>%
  # Sort data (makes it easier later)
  arrange(Sector)
  
# Jags data
dir_jags <- with(directors, list(## Outcome for individuals (level 1)
  
                                 compensation = Compensation,
                                 # Age of individuals
                                 age=Age,
                                 # Gender of individuals
                                 gender=Male,
                                 
                                 ## Company-level variables (level 2)
                                 
                                 # Company indicator
                                 company=as.numeric(as.factor(Company)),
                                 # Average age of directors for each company
                                 avgAge=avgAge,
                                 # Proportion of males for each company
                                 avgMale=propMale,
                                 
                                 ## Group totals
                                 
                                 # Number of individuals
                                 n=nrow(directors),  
                                 # Number of companies
                                 k=length(unique(Company)),
                                 
                                 ## Number of predictors
                                 
                                 # Number of individual-level predictors
                                 p1=2,
                                 # Number of company-level predictors
                                 p2=2))

# Intercept-only model
io_mod <- "model {

  ### Level 2 ==> companies

	# Priors (companies)
	tau_u0 ~ dgamma(.01, .01)
	# Hyperprior for mean
  gamma_00 ~ dnorm(0, 1.0E-4)
  # For each company
	for (j in 1:k) {
	  # For each company, draw from normal
    b0[j] ~ dnorm(gamma_00, tau_u0)
	}

	### Level 1 ==> individuals

  # Priors
  # Individual precision
	tau ~ dgamma(.01, .01)  # standard deviation of fixed effect (variance within companies)
  # For each individual
	for (i in 1:n) {
		compensation[i] ~ dnorm(mu[i], tau) # Combine likelihood and priors
		mu[i] <- b0[company[i]] # Linear combination for each person
	}

  # Invert gamma
  sigma_e <- 1 / sqrt(tau)
  sigma_u0 <- 1 / sqrt(tau_u0)

}"

# Run the model in JAGS
# Initial values
dir_inits <- list(
  init1 <- list(tau=runif(1), tau_u0=runif(1)),
  init2 <- list(tau=runif(1), tau_u0=runif(1))
)

# Specify model in JAGS
mod_io <- jags.model(textConnection(io_mod), 
                     data = dir_jags,
                     inits = dir_inits,
                     n.chains=2)

# Burn
update(mod_io, n.iter=60000)

# Draw samples
params <- c("sigma_e", "sigma_u0", "gamma_00")
# Run the chain
resm1 <- coda.samples(mod_io, variable.names = params, n.iter=500000, thin = 10)

# Convergence?
plot(resm1) # Ok
gelman.plot(resm1) # Ok

# DIC
DICm1 <- dic.samples(mod_io, thin=5, n.iter=20000, type="pD") # 288.6

# MAP values
MAPm1 <- apply(do.call(rbind.data.frame, resm1), 2, mean)

# Coerce to matrix
resmat <- do.call(rbind, resm1)

# New matrix with same dims
ve <- matrix(0, ncol=1, nrow=nrow(resmat))

# FOr each row
# Posterior distribution of ICC
for(i in seq_along(1:nrow(resmat))) {
  ve[i,] <- c(resmat[i,3]^2 / sum(resmat[i,-1]^2))
}

quantile(ve, c(0.025, .25, .5, .75, .975))
# VE between .45 and .69

# Save the model and DIC
saveRDS(resm1, "models/ML_interceptonly.rds")
saveRDS(DICm1, "models/ML_io_DIC.rds")
```

Given the hierarchical nature of the data, we next run a linear mixed effects model. The intra-class correlation coefficient shows that a large portion of the variance is found at the company level ($\rho = 57\%; \ 95\% CCI=[.451, .686]$), meaning that the expected correlation of two randomly picked directors from the same company is $\hat{r}=.573$.^[The complete multilevel analysis of this data can be found [here](https://github.com/JasperHG90/blm/blob/master/notes/6.%20Linear%20Mixed%20Effects%20in%20JAGS/multiJags.pdf).] The final model corresponds to the following equation.

$$
\begin{aligned}
\widehat{\log(\text{compensation}_i)} = 
&\gamma_{00} + u_{0j} + \gamma_{10}\cdot [\text{Age}_{ij} - \overline{\text{Age}}]) + e_{ij}
\end{aligned}
$$

Where $\gamma_{00}$ is the overall intercept and $u_{0j}$ is a company-specific error term. The DIC of $281$ indicates that the multilevel approach is appropriate, given that this model fits better than the regular regression model. The marginal and conditional R-squared values [@nakagawa2013general]^[The marginal R-squared takes into account only the fixed part of the variance, while the conditional R-squared takes into account the fixed and random parts. See [@nakagawa2013general]] are $R^2_{\text{M}} = .0056; \ 95\% \ \text{CCI}=[.0008, .015]$ and $R^2_{\text{C}} = .5398; \ 95\% \ \text{CCI}=[.4807, .6]$ respectively. These values suggest that the fixed part of the model (age) explains almost no variation in the data, but the fixed and random parts together explain some $54\%$ of the total variation. 

```{r MLM level-1 predictors}
# Initial values
dir_inits <- list(
  init1 <- list(tau=runif(1), tau_u0=runif(1)),
  init2 <- list(tau=runif(1), tau_u0=runif(1))
)

# Model
mlm_2 <- "model {

  ### Level 2 ==> companies

	# Priors (companies)
	tau_u0 ~ dgamma(.01, .01)
	# Hyperprior for mean
  gamma_00 ~ dnorm(0, 1.0E-4)
  gamma_10 ~ dnorm(0, 1.0E-4)
  # For each company
	for (j in 1:k) {
	  # For each company, draw from normal
    b0[j] ~ dnorm(gamma_00, tau_u0)
	}

	### Level 1 ==> individuals

  # Priors
  # Individual precision
	tau ~ dgamma(.01, .01)  # standard deviation of fixed effect
  # For each individual
	for (i in 1:n) {
		compensation[i] ~ dnorm(mu[i], tau) # Combine likelihood and priors
		mu[i] <- b0[company[i]] + # Intercept
		         gamma_10*age[i] #+ gamma_20*gender[i] # Level 1 variables
	}

  # Invert gamma
  sigma_e <- 1 / sqrt(tau)
  sigma_u0 <- 1 / sqrt(tau_u0)

}"

# Specify model in JAGS
mod_io <- jags.model(textConnection(mlm_2), 
                     data = dir_jags,
                     inits = dir_inits,
                     n.chains=2)

# Burn
update(mod_io, n.iter=60000)

# Draw samples
params <- c("sigma_e", "sigma_u0", "gamma_00", "gamma_10")
# Run the chain
resm2 <- coda.samples(mod_io, variable.names = params, n.iter=500000, thin = 10)

# Convergence?
plot(resm21) # Ok
gelman.plot(resm2) # Ok

# DIC
DICm2 <- dic.samples(mod_io, thin=5, n.iter=20000, type="pD") # 281 => +- 7 difference

# Calculate variance explained
# MAP values for model 1
MAPm1 <- apply(do.call(rbind.data.frame, resm1), 2, mean)
MAPm2 <- apply(do.call(rbind.data.frame, resm2), 2, mean)

# Map values
#MAPve <- apply(ve_2, 2, median)

# Save model
saveRDS(DICm2, "models/ML_final_DIC.rds")
saveRDS(resm2, "models/model3.rds")
```

```{r calculate variance explained for mixed model, eval=TRUE}
# Calculate R^2. Marginal (only explained by predictors) & Conditional (predicted by variance component)
# See: https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.2041-210x.2012.00261.x

# Function to calculate marginal & conditional R^2
# @param X design matrix containing fixed part of the model
# @param w coefficient matrix
# @e_1 random part of level 1
# @e_2 random part of level 2
MLM_R2 <- function(X, w, e_1, e_2) {
  
  # Component 1: fixed effect
  e_f <- var(X %*% w)

  # Compute marginal & conditional R^2
  mar <- (e_f) / (e_f + e_1 + e_2) # Variance explained by fixed factors (marginal R^2)
  con <- (e_f + e_2) / (e_f + e_1 + e_2) # Variance explained by fixed and random factors (conditional R^2)
  
  # Return
  return(
    c(mar, con)
  )
  
}

# Coerce to matrix
resmat <- do.call(rbind, resm2)

# New matrix with same dims
ve_2 <- matrix(0, ncol=2, nrow=nrow(resmat))

# X for fixed effect
X <- matrix(directors$Age, ncol=1)

# FOr each row
# Posterior distribution of variance explained (pseudo R-squared)
for(i in seq_along(1:nrow(resmat))) {
  ve_2[i,] <- MLM_R2(X, 
                 resmat[i,2],
                 resmat[i,3],
                 resmat[i,4])
}

# Median var explained
mve <- apply(ve_2, 2, median)
```

```{r r-squared plot, fig.cap="Bayesian R-squared value for model 1 (a) and model 2 (b). The proportion of cases in which the R-squared value of model 2 exceeds that of model 1 is .99. Figure (c) shows the marginal (blue) and conditional (red) R-squared values for the posterior distribution. The marginal R-squared indicates the amount of variance explained by the fixed part of the model; the conditional R-squared indicates the amount of variance explained by the fixed and random part of the model.", fig.height=2, fig.width=7, eval=TRUE}
library(latex2exp)
library(gridExtra)
library(ggplot2)
# Plot r-squared
# Model 1
p1 <- dirmod2 %>% get_value("rsq") %>% plot() + theme_blm(text_size = 8) +
  theme(axis.title = element_blank(),
        axis.text.y = element_blank(),
        plot.subtitle = element_blank()) +
  labs(title = latex2exp::TeX("\\textbf{(b) Bayesian $R^2$ posterior (model 2)}"))
# Model 2 (multilevel)
veg <- ve_2 %>%
  as.data.frame() %>%
  rename("MR2" = V1, "CR2" = V2) %>%
  #select(CR2) %>%
  tidyr::gather(type, rsq)
# Plot
p2 <-  ggplot(veg, aes(x=rsq, fill=type)) +
    geom_histogram(alpha=.4, color="black") +
    theme_blm(text_size=8) +
    scale_fill_brewer(palette="Set1") +
    geom_vline(xintercept=veg %>% filter(type=="MR2") %>% 
                 select(rsq) %>% pull() %>% 
                 median(), linetype="dashed") +
    geom_vline(xintercept=veg %>% filter(type=="CR2") %>% 
                 select(rsq) %>% pull() %>% 
                 median(), linetype="dashed") +
    theme(axis.title = element_blank(),
          axis.text.y = element_blank(),
          legend.position = "none") +
    labs(title = latex2exp::TeX("\\textbf{(c) Marginal and Conditional $R^2$ posterior}"))

# Grid
grid.arrange(p1, p2, ncol=2)
```

# Conclusion

The analysis shows that gender is not a predictor of compensation for independent directors. There are some clear limitations to this analysis. Firstly, I have to delete some $44\%$ of the data during missingness. Secondly, the results indicate that there is a lot of uncertainty in the estimation of the gender effect, which is exemplified by the large standard deviation of the posterior distribution. Furthermore, director compensation is best modeled using company-specific intercepts, but we do not have the right variables at either the company or individual level that would help us explain the heterogeneity in compensation among directors within boards and between boards. Future research should focus on collecting more complete data and other covariates such as the percentage of female directors.

\begin{table}[H] \centering 
\begin{small}
  \caption{Model results } 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{3}{c}{Compensation (GBR '000, logged)} \\ 
\\[-1.8ex] & (1) & (2) & (3)\\ 
\\[-1.8ex] & Intercept-only & Full model & Linear mixed effects \\
\\[-1.8ex] & (blm) & (blm) & (JAGS) \\
\hline \\[-1.8ex] 
\textit{(a) Fixed} & & & \\
Constant & 4.991 (4.935, 5.046) & 4.991 (4.936, 5.046) & 4.993 (4.875, 5.112) \\ 
Male & & .071 ($-$.075, .219) & \\ 
Age & & .008 ($-$.00, .017) & .009 (.003, .015) \\ 
\hline \\[-1.8ex] 
\textit{(b) Random} & & & \\
$\sigma^2_e$ & .52 (.482, .56) & .516 (.479, .558) & .342 (.315, .372) \\
$\sigma^2_{u0}$ & & & .399 (.322, .504) \\
\hline \\[-1.8ex] 
\textit{(c) Model Fit} \\
Observations & 336 & 336 & 336 \\
Companies & & & 52 \\
DIC & 513 & 409 & 281 \\ 
Penalty. & 2 & 4 & 49 \\ 
R$^{2}$ & & .021 & .006 (M), .536 (C) \\ 
\hline \\[-1.8ex] 
\textit{(d) Post. Pred. Checks} & & & \\
Normality &  & .362 &  \\
Homoskedasticity &  & .321 & \\
Independence &  & 0 & \\
\hline \\[-1.8ex] 
\textit{(e) Bayes' Factors (model 2 only)} & & & \\
Hypothesis & BF (complexity, fit) & PMPa & PMPb \\
\hline \\[-1.8ex] 
$\text{H}_1: \beta_\text{Male} > 0$ & 1.243 (.512, .566) & .403 & .295  \\
$\text{H}_2: \beta_{\text{Age}} > 0$ & 24.86 (.594, .972) & .597 & .438 \\
$\text{H}_u: \beta_{\text{Male}}, \beta_{\text{Age}}$ &  &  & .267 \\
\hline 
\end{tabular} 
\end{small}
\end{table}

# References
